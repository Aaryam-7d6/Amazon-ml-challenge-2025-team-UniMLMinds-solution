{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb02444d-a723-4431-b681-f16da034056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Device: cpu\n",
      "üìä Using log-transformed prices\n",
      "\n",
      "üìÇ Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 329\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Predicted price range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmission[\u001b[33m'\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmission[\u001b[33m'\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 213\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìÇ Loading data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m train_df = pd.read_csv(Config.TRAIN_PATH)\n\u001b[32m    214\u001b[39m test_df = pd.read_csv(Config.TEST_PATH)\n\u001b[32m    216\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\llm1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\llm1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = TextFileReader(filepath_or_buffer, **kwds)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\llm1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28mself\u001b[39m._make_engine(f, \u001b[38;5;28mself\u001b[39m.engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\llm1\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = get_handle(\n\u001b[32m   1881\u001b[39m     f,\n\u001b[32m   1882\u001b[39m     mode,\n\u001b[32m   1883\u001b[39m     encoding=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1884\u001b[39m     compression=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1885\u001b[39m     memory_map=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mmemory_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1886\u001b[39m     is_text=is_text,\n\u001b[32m   1887\u001b[39m     errors=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mencoding_errors\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1888\u001b[39m     storage_options=\u001b[38;5;28mself\u001b[39m.options.get(\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1889\u001b[39m )\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\llm1\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optimized DistilBERT Baseline for Amazon ML Challenge\n",
    "Text ‚Üí DistilBERT ‚Üí Dense Layers ‚Üí Price Prediction\n",
    "Target: Minimize SMAPE score\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    TRAIN_PATH = 'train.csv'\n",
    "    TEST_PATH = 'test.csv'\n",
    "    OUTPUT_PATH = 'submission.csv'\n",
    "    \n",
    "    # Model\n",
    "    MODEL_NAME = 'distilbert-base-uncased'  # Apache 2.0 license\n",
    "    MAX_LEN = 128  # Balance between speed and info retention\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 32  # Adjust based on GPU memory\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_RATIO = 0.1\n",
    "    \n",
    "    # Cross-validation\n",
    "    N_FOLDS = 5\n",
    "    SEED = 42\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Price transformation (key insight from data: price range 0.13 to 2796)\n",
    "    USE_LOG_PRICE = True  # Log transform stabilizes training\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS\n",
    "# ============================================================================\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error (lower is better)\n",
    "    Formula: (1/n) * Œ£(|y_pred - y_true| / ((|y_pred| + |y_true|) / 2))\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_pred) + np.abs(y_true)) / 2\n",
    "    # Avoid division by zero\n",
    "    denominator = np.where(denominator == 0, 1e-8, denominator)\n",
    "    return np.mean(numerator / denominator)\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET\n",
    "# ============================================================================\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, texts, prices=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.prices = prices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        if self.prices is not None:\n",
    "            item['price'] = torch.tensor(self.prices[idx], dtype=torch.float)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class DistilBERTPricePredictor(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pretrained DistilBERT\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Regression head with progressive dimension reduction\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT output\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Predict price\n",
    "        price = self.regressor(cls_output)\n",
    "        \n",
    "        return price.squeeze()\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        prices = batch['price'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Huber Loss (robust to outliers, better than MSE for price prediction)\n",
    "        loss = nn.HuberLoss(delta=1.0)(predictions, prices)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, device, scaler=None):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Validation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            prices = batch['price'].to(device)\n",
    "            \n",
    "            preds = model(input_ids, attention_mask)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actuals.extend(prices.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    # Inverse transform if using log prices\n",
    "    if Config.USE_LOG_PRICE:\n",
    "        predictions = np.expm1(predictions)  # exp(x) - 1\n",
    "        actuals = np.expm1(actuals)\n",
    "    \n",
    "    # Ensure positive predictions\n",
    "    predictions = np.maximum(predictions, 0.01)\n",
    "    \n",
    "    # Calculate SMAPE\n",
    "    smape_score = smape(actuals, predictions)\n",
    "    \n",
    "    return smape_score, predictions, actuals\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "def main():\n",
    "    print(f\"üöÄ Device: {Config.DEVICE}\")\n",
    "    print(f\"üìä Using {'log-transformed' if Config.USE_LOG_PRICE else 'raw'} prices\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"üìÇ Loading data...\")\n",
    "    train_df = pd.read_csv(Config.TRAIN_PATH)\n",
    "    test_df = pd.read_csv(Config.TEST_PATH)\n",
    "    \n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Test shape: {test_df.shape}\")\n",
    "    print(f\"Price range: {train_df['price'].min():.2f} - {train_df['price'].max():.2f}\\n\")\n",
    "    \n",
    "    # Prepare prices\n",
    "    prices = train_df['price'].values\n",
    "    if Config.USE_LOG_PRICE:\n",
    "        prices = np.log1p(prices)  # log(1 + x) to handle prices close to 0\n",
    "        print(\"‚úÖ Applied log1p transformation to prices\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    print(\"üî§ Loading tokenizer...\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    \n",
    "    # K-Fold Cross-Validation\n",
    "    kfold = KFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\n",
    "    fold_scores = []\n",
    "    test_predictions = np.zeros(len(test_df))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_df)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîÑ FOLD {fold + 1}/{Config.N_FOLDS}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Split data\n",
    "        train_texts = train_df.iloc[train_idx]['catalog_content'].values\n",
    "        train_prices = prices[train_idx]\n",
    "        val_texts = train_df.iloc[val_idx]['catalog_content'].values\n",
    "        val_prices = prices[val_idx]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = PriceDataset(train_texts, train_prices, tokenizer, Config.MAX_LEN)\n",
    "        val_dataset = PriceDataset(val_texts, val_prices, tokenizer, Config.MAX_LEN)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = DistilBERTPricePredictor(Config.MODEL_NAME).to(Config.DEVICE)\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n",
    "        \n",
    "        total_steps = len(train_loader) * Config.EPOCHS\n",
    "        warmup_steps = int(total_steps * Config.WARMUP_RATIO)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        \n",
    "        # Training loop\n",
    "        best_smape = float('inf')\n",
    "        patience = 0\n",
    "        max_patience = 2\n",
    "        \n",
    "        for epoch in range(Config.EPOCHS):\n",
    "            print(f\"\\nüìà Epoch {epoch + 1}/{Config.EPOCHS}\")\n",
    "            \n",
    "            train_loss = train_epoch(model, train_loader, optimizer, scheduler, Config.DEVICE)\n",
    "            val_smape, _, _ = validate_epoch(model, val_loader, Config.DEVICE)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f} | Val SMAPE: {val_smape:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_smape < best_smape:\n",
    "                best_smape = val_smape\n",
    "                torch.save(model.state_dict(), f'best_model_fold{fold}.pt')\n",
    "                patience = 0\n",
    "                print(f\"‚úÖ Best model saved! SMAPE: {best_smape:.4f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(f\"‚ö†Ô∏è Early stopping triggered\")\n",
    "                    break\n",
    "        \n",
    "        fold_scores.append(best_smape)\n",
    "        print(f\"\\nüéØ Fold {fold + 1} Best SMAPE: {best_smape:.4f}\")\n",
    "        \n",
    "        # Predict on test set\n",
    "        model.load_state_dict(torch.load(f'best_model_fold{fold}.pt'))\n",
    "        test_dataset = PriceDataset(test_df['catalog_content'].values, None, tokenizer, Config.MAX_LEN)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "        \n",
    "        model.eval()\n",
    "        fold_test_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=f'Predicting test (Fold {fold+1})'):\n",
    "                input_ids = batch['input_ids'].to(Config.DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(Config.DEVICE)\n",
    "                preds = model(input_ids, attention_mask)\n",
    "                fold_test_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "        fold_test_preds = np.array(fold_test_preds)\n",
    "        if Config.USE_LOG_PRICE:\n",
    "            fold_test_preds = np.expm1(fold_test_preds)\n",
    "        fold_test_preds = np.maximum(fold_test_preds, 0.01)\n",
    "        \n",
    "        test_predictions += fold_test_preds / Config.N_FOLDS\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üèÜ FINAL RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Average SMAPE across folds: {np.mean(fold_scores):.4f} ¬± {np.std(fold_scores):.4f}\")\n",
    "    print(f\"Fold scores: {[f'{s:.4f}' for s in fold_scores]}\")\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'sample_id': test_df['sample_id'],\n",
    "        'price': test_predictions\n",
    "    })\n",
    "    submission.to_csv(Config.OUTPUT_PATH, index=False)\n",
    "    print(f\"\\n‚úÖ Submission saved to {Config.OUTPUT_PATH}\")\n",
    "    print(f\"üìä Predicted price range: {submission['price'].min():.2f} - {submission['price'].max():.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869c2e4-12a8-4ae1-a682-b514222e850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DistilBERT Price Prediction Baseline - Amazon ML Challenge\n",
    "Formatted for competition submission structure\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    MODEL_NAME = 'distilbert-base-uncased'\n",
    "    MAX_LEN = 128\n",
    "    BATCH_SIZE = 32\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    USE_LOG_PRICE = True\n",
    "    MODEL_PATH = 'trained_model.pth'  # Path to save/load trained model\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET\n",
    "# ============================================================================\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, texts, prices=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.prices = prices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        if self.prices is not None:\n",
    "            item['price'] = torch.tensor(self.prices[idx], dtype=torch.float)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class DistilBERTPricePredictor(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        price = self.regressor(cls_output)\n",
    "        return price.squeeze()\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "def train_model(train_df):\n",
    "    \"\"\"\n",
    "    Train the DistilBERT model on training data\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Model Training...\")\n",
    "    print(f\"Device: {Config.DEVICE}\")\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    texts = train_df['catalog_content'].values\n",
    "    prices = train_df['price'].values\n",
    "    \n",
    "    if Config.USE_LOG_PRICE:\n",
    "        prices = np.log1p(prices)\n",
    "        print(\"‚úÖ Applied log1p transformation to prices\")\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    model = DistilBERTPricePredictor(Config.MODEL_NAME).to(Config.DEVICE)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = PriceDataset(texts, prices, tokenizer, Config.MAX_LEN)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    loss_fn = nn.HuberLoss(delta=1.0)\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 3  # Reduced for faster training\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nüìà Epoch {epoch + 1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(Config.DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(Config.DEVICE)\n",
    "            target_prices = batch['price'].to(Config.DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(predictions, target_prices)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': {\n",
    "            'model_name': Config.MODEL_NAME,\n",
    "            'use_log_price': Config.USE_LOG_PRICE\n",
    "        }\n",
    "    }, Config.MODEL_PATH)\n",
    "    print(f\"\\n‚úÖ Model saved to {Config.MODEL_PATH}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# PREDICTION FUNCTION (COMPETITION FORMAT)\n",
    "# ============================================================================\n",
    "def predictor(sample_id, catalog_content, image_link, model=None, tokenizer=None):\n",
    "    '''\n",
    "    Predict product price using DistilBERT model\n",
    "    \n",
    "    Parameters:\n",
    "    - sample_id: Unique identifier for the sample\n",
    "    - catalog_content: Text containing product title and description\n",
    "    - image_link: URL to product image (not used in baseline)\n",
    "    \n",
    "    Returns:\n",
    "    - price: Predicted price as a float\n",
    "    '''\n",
    "    if model is None or tokenizer is None:\n",
    "        raise ValueError(\"Model and tokenizer must be provided\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    encoding = tokenizer(\n",
    "        str(catalog_content),\n",
    "        max_length=Config.MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(Config.DEVICE)\n",
    "    attention_mask = encoding['attention_mask'].to(Config.DEVICE)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_ids, attention_mask)\n",
    "    \n",
    "    # Inverse transform\n",
    "    price = prediction.cpu().item()\n",
    "    if Config.USE_LOG_PRICE:\n",
    "        price = np.expm1(price)  # exp(x) - 1\n",
    "    \n",
    "    # Ensure positive price\n",
    "    price = max(price, 0.01)\n",
    "    \n",
    "    return round(price, 2)\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PREDICTION (OPTIMIZED FOR SPEED)\n",
    "# ============================================================================\n",
    "def batch_predictor(test_df, model, tokenizer, batch_size=32):\n",
    "    \"\"\"\n",
    "    Predict prices for entire test set in batches (faster than row-by-row)\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÆ Generating Predictions...\")\n",
    "    \n",
    "    texts = test_df['catalog_content'].values\n",
    "    test_dataset = PriceDataset(texts, None, tokenizer, Config.MAX_LEN)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(Config.DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(Config.DEVICE)\n",
    "            \n",
    "            preds = model(input_ids, attention_mask)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Inverse transform\n",
    "    if Config.USE_LOG_PRICE:\n",
    "        predictions = np.expm1(predictions)\n",
    "    \n",
    "    # Ensure positive prices\n",
    "    predictions = np.maximum(predictions, 0.01)\n",
    "    predictions = np.round(predictions, 2)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_FOLDER = 'dataset/'\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DistilBERT Price Prediction - Amazon ML Challenge\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if model already trained\n",
    "    if os.path.exists(Config.MODEL_PATH):\n",
    "        print(f\"\\n‚úÖ Found existing model at {Config.MODEL_PATH}\")\n",
    "        print(\"Loading trained model...\")\n",
    "        \n",
    "        # Load model\n",
    "        checkpoint = torch.load(Config.MODEL_PATH, map_location=Config.DEVICE)\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        model = DistilBERTPricePredictor(Config.MODEL_NAME).to(Config.DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No trained model found. Training new model...\")\n",
    "        \n",
    "        # Load training data\n",
    "        train_path = os.path.join(DATASET_FOLDER, 'train.csv')\n",
    "        if not os.path.exists(train_path):\n",
    "            raise FileNotFoundError(f\"Training data not found at {train_path}\")\n",
    "        \n",
    "        train_df = pd.read_csv(train_path)\n",
    "        print(f\"Loaded {len(train_df)} training samples\")\n",
    "        \n",
    "        # Train model\n",
    "        model, tokenizer = train_model(train_df)\n",
    "    \n",
    "    # Load test data\n",
    "    print(f\"\\nüìÇ Loading test data...\")\n",
    "    test_path = os.path.join(DATASET_FOLDER, 'test.csv')\n",
    "    test = pd.read_csv(test_path)\n",
    "    print(f\"Loaded {len(test)} test samples\")\n",
    "    \n",
    "    # METHOD 1: Batch Prediction (FASTER - RECOMMENDED)\n",
    "    print(\"\\nüöÄ Using batch prediction for speed...\")\n",
    "    test['price'] = batch_predictor(test, model, tokenizer, batch_size=Config.BATCH_SIZE)\n",
    "    \n",
    "    # METHOD 2: Row-by-row prediction (slower, but matches competition format)\n",
    "    # Uncomment below if you want to use row-by-row approach\n",
    "    \"\"\"\n",
    "    print(\"\\nüöÄ Using row-by-row prediction...\")\n",
    "    test['price'] = test.apply(\n",
    "        lambda row: predictor(\n",
    "            row['sample_id'], \n",
    "            row['catalog_content'], \n",
    "            row['image_link'],\n",
    "            model=model,\n",
    "            tokenizer=tokenizer\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select only required columns\n",
    "    output_df = test[['sample_id', 'price']]\n",
    "    \n",
    "    # Save predictions\n",
    "    output_filename = os.path.join(DATASET_FOLDER, 'test_out.csv')\n",
    "    output_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ PREDICTION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìÅ Predictions saved to: {output_filename}\")\n",
    "    print(f\"üìä Total predictions: {len(output_df)}\")\n",
    "    print(f\"üí∞ Price range: ‚Çπ{output_df['price'].min():.2f} - ‚Çπ{output_df['price'].max():.2f}\")\n",
    "    print(f\"üìà Average price: ‚Çπ{output_df['price'].mean():.2f}\")\n",
    "    print(f\"\\nüîç Sample predictions:\")\n",
    "    print(output_df.head(10))\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5596c-294b-45d2-a6f2-de39c7fd4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPU-OPTIMIZED DistilBERT Training for RTX 3050 (4GB VRAM)\n",
    "Maximizes throughput with mixed precision, gradient accumulation, and efficient data loading\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "import torch\n",
    "# ... rest of imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED CONFIGURATION FOR RTX 3050\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    # Model\n",
    "    MODEL_NAME = 'distilbert-base-uncased'\n",
    "    MAX_LEN = 128\n",
    "    \n",
    "    # GPU Optimization (RTX 3050 specific)\n",
    "    BATCH_SIZE = 48              # Increased from 32 (uses ~3.8GB)\n",
    "    ACCUMULATION_STEPS = 1       # Set to 2 if OOM occurs\n",
    "    USE_MIXED_PRECISION = True   # FP16 training (50% faster)\n",
    "    \n",
    "    # DataLoader optimization\n",
    "    NUM_WORKERS = 4              # CPU threads for data loading\n",
    "    PIN_MEMORY = True            # Faster CPU-GPU transfer\n",
    "    PREFETCH_FACTOR = 2          # Prefetch batches\n",
    "    \n",
    "    # Training\n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_RATIO = 0.1\n",
    "    \n",
    "    # Other\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    USE_LOG_PRICE = True\n",
    "    MODEL_PATH = 'trained_model.pth'\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET (WITH OPTIMIZATIONS)\n",
    "# ============================================================================\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, texts, prices=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.prices = prices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        if self.prices is not None:\n",
    "            item['price'] = torch.tensor(self.prices[idx], dtype=torch.float)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class DistilBERTPricePredictor(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        price = self.regressor(cls_output)\n",
    "        return price.squeeze()\n",
    "\n",
    "# ============================================================================\n",
    "# GPU-OPTIMIZED TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "def train_model_optimized(train_df):\n",
    "    \"\"\"\n",
    "    GPU-optimized training with mixed precision and efficient data loading\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting GPU-Optimized Training...\")\n",
    "    print(f\"Device: {Config.DEVICE}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
    "    print(f\"VRAM Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
    "    print(f\"Mixed precision: {Config.USE_MIXED_PRECISION}\")\n",
    "    print(f\"Gradient accumulation: {Config.ACCUMULATION_STEPS}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    texts = train_df['catalog_content'].values\n",
    "    prices = train_df['price'].values\n",
    "    \n",
    "    if Config.USE_LOG_PRICE:\n",
    "        prices = np.log1p(prices)\n",
    "        print(\"‚úÖ Applied log1p transformation to prices\")\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    print(\"\\nüî§ Loading tokenizer and model...\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    model = DistilBERTPricePredictor(Config.MODEL_NAME).to(Config.DEVICE)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Create optimized dataset and dataloader\n",
    "    train_dataset = PriceDataset(texts, prices, tokenizer, Config.MAX_LEN)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=Config.PIN_MEMORY,\n",
    "        prefetch_factor=Config.PREFETCH_FACTOR,\n",
    "        persistent_workers=True  # Keep workers alive between epochs\n",
    "    )\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=Config.LEARNING_RATE,\n",
    "        weight_decay=Config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_loader) * Config.EPOCHS // Config.ACCUMULATION_STEPS\n",
    "    warmup_steps = int(total_steps * Config.WARMUP_RATIO)\n",
    "    \n",
    "    from transformers import get_cosine_schedule_with_warmup\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = nn.HuberLoss(delta=1.0)\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler() if Config.USE_MIXED_PRECISION else None\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\nüìà Starting training for {Config.EPOCHS} epochs...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{Config.EPOCHS}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            input_ids = batch['input_ids'].to(Config.DEVICE, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(Config.DEVICE, non_blocking=True)\n",
    "            target_prices = batch['price'].to(Config.DEVICE, non_blocking=True)\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            if Config.USE_MIXED_PRECISION:\n",
    "                with autocast():\n",
    "                    predictions = model(input_ids, attention_mask)\n",
    "                    loss = loss_fn(predictions, target_prices)\n",
    "                    loss = loss / Config.ACCUMULATION_STEPS\n",
    "                \n",
    "                # Scaled backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient accumulation\n",
    "                if (batch_idx + 1) % Config.ACCUMULATION_STEPS == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                # Standard precision\n",
    "                predictions = model(input_ids, attention_mask)\n",
    "                loss = loss_fn(predictions, target_prices)\n",
    "                loss = loss / Config.ACCUMULATION_STEPS\n",
    "                loss.backward()\n",
    "                \n",
    "                if (batch_idx + 1) % Config.ACCUMULATION_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * Config.ACCUMULATION_STEPS\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item() * Config.ACCUMULATION_STEPS:.4f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                'gpu_mem': f'{torch.cuda.memory_allocated() / 1e9:.1f}GB'\n",
    "            })\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Peak GPU Memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        # Reset peak memory stats\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Save model\n",
    "    print(f\"\\nüíæ Saving model...\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': {\n",
    "            'model_name': Config.MODEL_NAME,\n",
    "            'use_log_price': Config.USE_LOG_PRICE,\n",
    "            'max_len': Config.MAX_LEN\n",
    "        }\n",
    "    }, Config.MODEL_PATH)\n",
    "    print(f\"‚úÖ Model saved to {Config.MODEL_PATH}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED BATCH PREDICTION\n",
    "# ============================================================================\n",
    "def batch_predictor_optimized(test_df, model, tokenizer):\n",
    "    \"\"\"\n",
    "    GPU-optimized batch prediction with mixed precision\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÆ Generating Predictions (GPU-Optimized)...\")\n",
    "    \n",
    "    texts = test_df['catalog_content'].values\n",
    "    test_dataset = PriceDataset(texts, None, tokenizer, Config.MAX_LEN)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=Config.BATCH_SIZE * 2,  # Double batch size for inference\n",
    "        shuffle=False,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=Config.PIN_MEMORY,\n",
    "        prefetch_factor=Config.PREFETCH_FACTOR\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(Config.DEVICE, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(Config.DEVICE, non_blocking=True)\n",
    "            \n",
    "            # Use mixed precision for inference too\n",
    "            if Config.USE_MIXED_PRECISION:\n",
    "                with autocast():\n",
    "                    preds = model(input_ids, attention_mask)\n",
    "            else:\n",
    "                preds = model(input_ids, attention_mask)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Inverse transform\n",
    "    if Config.USE_LOG_PRICE:\n",
    "        predictions = np.expm1(predictions)\n",
    "    \n",
    "    # Ensure positive prices\n",
    "    predictions = np.maximum(predictions, 0.01)\n",
    "    predictions = np.round(predictions, 2)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_FOLDER = 'dataset/'\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"GPU-OPTIMIZED DistilBERT Price Prediction\")\n",
    "    print(\"Optimized for NVIDIA GeForce RTX 3050\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check GPU availability\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ö†Ô∏è WARNING: CUDA not available! Running on CPU (very slow)\")\n",
    "    else:\n",
    "        print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Check if model already trained\n",
    "    if os.path.exists(Config.MODEL_PATH):\n",
    "        print(f\"\\n‚úÖ Found existing model at {Config.MODEL_PATH}\")\n",
    "        print(\"Loading trained model...\")\n",
    "        \n",
    "        checkpoint = torch.load(Config.MODEL_PATH, map_location=Config.DEVICE)\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "        model = DistilBERTPricePredictor(Config.MODEL_NAME).to(Config.DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No trained model found. Training new model...\")\n",
    "        \n",
    "        train_path = os.path.join(DATASET_FOLDER, 'train.csv')\n",
    "        if not os.path.exists(train_path):\n",
    "            raise FileNotFoundError(f\"Training data not found at {train_path}\")\n",
    "        \n",
    "        print(f\"üìÇ Loading training data from {train_path}\")\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        print(f\"‚úÖ Loaded {len(train_df)} training samples\")\n",
    "        \n",
    "        # Train model with GPU optimization\n",
    "        model, tokenizer = train_model_optimized(train_df)\n",
    "    \n",
    "    # Load test data\n",
    "    print(f\"\\nüìÇ Loading test data...\")\n",
    "    test_path = os.path.join(DATASET_FOLDER, 'test.csv')\n",
    "    test = pd.read_csv(test_path)\n",
    "    print(f\"‚úÖ Loaded {len(test)} test samples\")\n",
    "    \n",
    "    # GPU-optimized batch prediction\n",
    "    test['price'] = batch_predictor_optimized(test, model, tokenizer)\n",
    "    \n",
    "    # Select only required columns\n",
    "    output_df = test[['sample_id', 'price']]\n",
    "    \n",
    "    # Save predictions\n",
    "    output_filename = os.path.join(DATASET_FOLDER, 'test_out.csv')\n",
    "    output_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ PREDICTION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìÅ Predictions saved to: {output_filename}\")\n",
    "    print(f\"üìä Total predictions: {len(output_df)}\")\n",
    "    print(f\"üí∞ Price range: ‚Çπ{output_df['price'].min():.2f} - ‚Çπ{output_df['price'].max():.2f}\")\n",
    "    print(f\"üìà Average price: ‚Çπ{output_df['price'].mean():.2f}\")\n",
    "    print(f\"üìâ Median price: ‚Çπ{output_df['price'].median():.2f}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nüéÆ GPU Stats:\")\n",
    "        print(f\"  Peak VRAM Used: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"  Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\nüîç Sample predictions:\")\n",
    "    print(output_df.head(10))\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84311af7-24b0-4671-b087-74183fe2ff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DistilBERT Price Prediction (25k Subset - Optimized GPU)\n",
      "============================================================\n",
      "üìä Train: 75000 | Test: 75000\n",
      "üöÄ Training Optimized DistilBERT Model...\n",
      "Device: cpu\n",
      "‚úÖ Applied log1p transformation to prices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|                                                                               | 0/391 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optimized DistilBERT Price Prediction Baseline (25k subset)\n",
    "GPU-Accelerated for RTX 3050\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    MODEL_NAME = 'distilbert-base-uncased'\n",
    "    MAX_LEN = 128\n",
    "    BATCH_SIZE = 64           # larger batch to maximize GPU throughput\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    USE_LOG_PRICE = True\n",
    "    EPOCHS = 2                # fewer epochs for quick iteration\n",
    "    MODEL_PATH = 'trained_model_25k.pth'\n",
    "    TRAIN_SIZE = 25000\n",
    "    TEST_SIZE = 25000\n",
    "    MIXED_PRECISION = True    # Enable AMP (Automatic Mixed Precision)\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET\n",
    "# ============================================================================\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, texts, prices=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.prices = prices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        if self.prices is not None:\n",
    "            item['price'] = torch.tensor(self.prices[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class DistilBERTPricePredictor(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        price = self.regressor(cls_output)\n",
    "        return price.squeeze()\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "def train_model(train_df):\n",
    "    print(\"üöÄ Training Optimized DistilBERT Model...\")\n",
    "    print(f\"Device: {Config.DEVICE}\")\n",
    "    \n",
    "    # Subset 25k samples\n",
    "    train_df = train_df.sample(n=Config.TRAIN_SIZE, random_state=42).reset_index(drop=True)\n",
    "    texts = train_df['catalog_content'].values\n",
    "    prices = train_df['price'].values\n",
    "\n",
    "    if Config.USE_LOG_PRICE:\n",
    "        prices = np.log1p(prices)\n",
    "        print(\"‚úÖ Applied log1p transformation to prices\")\n",
    "\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(Config.MODEL_NAME)\n",
    "    model = DistilBERTPricePredictor(Config.MODEL_NAME).to(Config.DEVICE)\n",
    "\n",
    "    train_dataset = PriceDataset(texts, prices, tokenizer, Config.MAX_LEN)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "    loss_fn = nn.HuberLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=Config.MIXED_PRECISION)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{Config.EPOCHS}\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(Config.DEVICE, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(Config.DEVICE, non_blocking=True)\n",
    "            target_prices = batch['price'].to(Config.DEVICE, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=Config.MIXED_PRECISION):\n",
    "                predictions = model(input_ids, attention_mask)\n",
    "                loss = loss_fn(predictions, target_prices)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"üìâ Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), Config.MODEL_PATH)\n",
    "    print(f\"‚úÖ Model saved to {Config.MODEL_PATH}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# PREDICTION\n",
    "# ============================================================================\n",
    "def batch_predictor(test_df, model, tokenizer):\n",
    "    test_df = test_df.sample(n=Config.TEST_SIZE, random_state=42).reset_index(drop=True)\n",
    "    texts = test_df['catalog_content'].values\n",
    "\n",
    "    test_dataset = PriceDataset(texts, None, tokenizer, Config.MAX_LEN)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(Config.DEVICE, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(Config.DEVICE, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=Config.MIXED_PRECISION):\n",
    "                preds = model(input_ids, attention_mask)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    if Config.USE_LOG_PRICE:\n",
    "        predictions = np.expm1(predictions)\n",
    "    predictions = np.maximum(predictions, 0.01)\n",
    "    predictions = np.round(predictions, 2)\n",
    "    return predictions\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_FOLDER = 'dataset/'\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"DistilBERT Price Prediction (25k Subset - Optimized GPU)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    train_path = os.path.join(DATASET_FOLDER, 'train.csv')\n",
    "    test_path = os.path.join(DATASET_FOLDER, 'test.csv')\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    print(f\"üìä Train: {len(train_df)} | Test: {len(test_df)}\")\n",
    "\n",
    "    model, tokenizer = train_model(train_df)\n",
    "    preds = batch_predictor(test_df, model, tokenizer)\n",
    "    test_df = test_df.iloc[:len(preds)].copy()\n",
    "    test_df['price'] = preds\n",
    "\n",
    "    output_path = os.path.join(DATASET_FOLDER, 'test_out_25k.csv')\n",
    "    test_df[['sample_id', 'price']].to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Predictions saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cee53-d9f0-4119-88d0-460a469ef4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a0bff-686e-4016-ad4a-ee0ce123545b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm1",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
